{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense,LSTM,Embedding,Bidirectional\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the dataset\n",
    "data=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping the rows which has the null values\n",
    "newData = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18285 entries, 0 to 20799\n",
      "Data columns (total 5 columns):\n",
      "id        18285 non-null bool\n",
      "title     18285 non-null bool\n",
      "author    18285 non-null bool\n",
      "text      18285 non-null bool\n",
      "label     18285 non-null bool\n",
      "dtypes: bool(5)\n",
      "memory usage: 232.1 KB\n"
     ]
    }
   ],
   "source": [
    "# taking the information of the null values\n",
    "newData.isnull().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the index of the dataframe\n",
    "newData.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the stopwords and the Snowball stemmer\n",
    "stop = stopwords.words('english')\n",
    "stem = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tasti'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem.stem('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 18285/18285 [00:07<00:00, 2596.43it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = []\n",
    "for i in tqdm(range(0,len(newData['title']))):\n",
    "    # removing all the function excdept the charecter for the each title\n",
    "    reviews = re.sub('[^a-zA-Z]',' ',newData['title'][i])\n",
    "    # converting the each title to the lower case\n",
    "    reviews = reviews.lower()\n",
    "    # splitting the each title based on the space\n",
    "    reviews = reviews.split()\n",
    "    \n",
    "    # removing the stopwords and stemming for each word of each title\n",
    "    reviews = [stem.stem(words) for words in reviews if not words in stop]\n",
    "    # joining each word of the title\n",
    "    reviews=' '.join(reviews)\n",
    "    # joing all the title\n",
    "    corpus.append(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding each title and listing all the one hot encoded title\n",
    "oneHotData=[one_hot(dat,5000) for dat in corpus ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding alll the sequence\n",
    "# pad_sequence( data , maxium length it should padd , where the padding should done)\n",
    "paddedData = pad_sequences(oneHotData,maxlen=40,padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(paddedData,newData['label'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the Sequential model\n",
    "model=Sequential()\n",
    "#creating the embedding layer \n",
    "# Embedding(input rows length,output length ,inut column length)\n",
    "model.add(Embedding(18285,32,input_length=40))\n",
    "# using the bidirectional LSTM\n",
    "# for getting the bidirectional LSTM we using the bidirectional and the LSTM\n",
    "# this has the 100 layers\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "# output sigmoid unit\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "# compile(which optimizer want to use,)\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sheik\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12799 samples, validate on 5486 samples\n",
      "Epoch 1/10\n",
      "12799/12799 [==============================] - 89s 7ms/step - loss: 0.4516 - accuracy: 0.7753 - val_loss: 0.2350 - val_accuracy: 0.9048\n",
      "Epoch 2/10\n",
      "12799/12799 [==============================] - 86s 7ms/step - loss: 0.1670 - accuracy: 0.9330 - val_loss: 0.1936 - val_accuracy: 0.9182\n",
      "Epoch 3/10\n",
      "12799/12799 [==============================] - 92s 7ms/step - loss: 0.1121 - accuracy: 0.9591 - val_loss: 0.2014 - val_accuracy: 0.9222\n",
      "Epoch 4/10\n",
      "12799/12799 [==============================] - 88s 7ms/step - loss: 0.0830 - accuracy: 0.9720 - val_loss: 0.2515 - val_accuracy: 0.9085\n",
      "Epoch 5/10\n",
      "12799/12799 [==============================] - 88s 7ms/step - loss: 0.0606 - accuracy: 0.9807 - val_loss: 0.2540 - val_accuracy: 0.9162\n",
      "Epoch 6/10\n",
      "12799/12799 [==============================] - 87s 7ms/step - loss: 0.0479 - accuracy: 0.9851 - val_loss: 0.2915 - val_accuracy: 0.9185\n",
      "Epoch 7/10\n",
      "12799/12799 [==============================] - 87s 7ms/step - loss: 0.0488 - accuracy: 0.9859 - val_loss: 0.2134 - val_accuracy: 0.9189\n",
      "Epoch 8/10\n",
      "12799/12799 [==============================] - 88s 7ms/step - loss: 0.0414 - accuracy: 0.9901 - val_loss: 0.3165 - val_accuracy: 0.9136\n",
      "Epoch 9/10\n",
      "12799/12799 [==============================] - 87s 7ms/step - loss: 0.0190 - accuracy: 0.9960 - val_loss: 0.3736 - val_accuracy: 0.9143\n",
      "Epoch 10/10\n",
      "12799/12799 [==============================] - 87s 7ms/step - loss: 0.0114 - accuracy: 0.9976 - val_loss: 0.4171 - val_accuracy: 0.9101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2f9b58a308>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain,ytrain,batch_size=128,epochs=10,validation_data=(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
